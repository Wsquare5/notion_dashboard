#!/usr/bin/env python3
"""
æ¯æ—¥è¡Œæƒ…æ€»ç»“è„šæœ¬
åŠŸèƒ½ï¼š
1. ä»ä¸»æ•°æ®åº“è¯»å–æ‰€æœ‰å¸ç§çš„å½“å‰æ•°æ®
2. ç­›é€‰å‡ºæ¶¨è·Œå¹…å‰5åå’Œå5å
3. å†™å…¥"æ¯æ—¥è¡Œæƒ…"æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•ï¼š
python3 scripts/daily_market_summary.py
"""

import json
import sys
from pathlib import Path
from datetime import datetime
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from typing import List, Dict

# é…ç½®æ–‡ä»¶è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
CONFIG_FILE = BASE_DIR / "config" / "config.json"
DAILY_MARKET_CONFIG = BASE_DIR / "config" / "daily_market_config.json"


def load_config():
    """åŠ è½½é…ç½®"""
    with CONFIG_FILE.open('r') as f:
        config = json.load(f)
    
    # åŠ è½½æ¯æ—¥è¡Œæƒ…æ•°æ®åº“é…ç½®
    if DAILY_MARKET_CONFIG.exists():
        with DAILY_MARKET_CONFIG.open('r') as f:
            daily_config = json.load(f)
            config['daily_market_database_id'] = daily_config.get('database_id')
    else:
        print("âš ï¸  æœªæ‰¾åˆ°æ¯æ—¥è¡Œæƒ…æ•°æ®åº“é…ç½®ï¼")
        print(f"è¯·åˆ›å»ºé…ç½®æ–‡ä»¶ï¼š{DAILY_MARKET_CONFIG}")
        print("æ ¼å¼ï¼š")
        print('{')
        print('  "database_id": "your_daily_market_database_id"')
        print('}')
        sys.exit(1)
    
    return config


def get_all_symbols_from_notion(notion_token: str, database_id: str) -> tuple:
    """ä»ä¸»æ•°æ®åº“è¯»å–æ‰€æœ‰å¸ç§æ•°æ®"""
    print("ğŸ“¥ æ­£åœ¨è¯»å–ä¸»æ•°æ®åº“...")
    
    headers = {
        "Authorization": f"Bearer {notion_token}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }
    
    url = f"https://api.notion.com/v1/databases/{database_id}/query"

    # Build a session that does not trust environment proxies and has retries
    session = requests.Session()
    session.headers.update(headers)
    session.trust_env = False  # ignore system/OS proxy env vars to avoid unexpected proxy use

    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=(429, 502, 503, 504),
        allowed_methods=("GET", "POST")
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("https://", adapter)
    session.mount("http://", adapter)

    all_pages = []
    has_more = True
    start_cursor = None

    try:
        while has_more:
            payload = {}
            if start_cursor:
                payload["start_cursor"] = start_cursor

            # Use session.post which will automatically retry per strategy
            try:
                resp = session.post(url, json=payload, timeout=30)
                resp.raise_for_status()
            except requests.exceptions.ProxyError as e:
                # Proxy errors are often environmental (VPN/proxy/firewall). If we have some pages
                # already collected return them with a warning; otherwise re-raise for visibility.
                if all_pages:
                    print(f"âš ï¸  Warning: Notion proxy error during pagination, returning {len(all_pages)} pages collected so far: {e}")
                    return all_pages
                else:
                    raise
            except requests.exceptions.RequestException as e:
                # For other request errors, try to surface a helpful message
                if all_pages:
                    print(f"âš ï¸  Warning: Notion request failed, returning {len(all_pages)} pages collected so far: {e}")
                    return all_pages
                else:
                    raise

            data = resp.json()
            all_pages.extend(data.get('results', []))
            has_more = data.get('has_more', False)
            start_cursor = data.get('next_cursor')

    except Exception as exc:
        # Bubble up a clearer error for the caller if nothing was collected
        print(f"\nâŒ æ— æ³•ä» Notion è¯»å–é¡µé¢: {exc}")
        print("å»ºè®®æ£€æŸ¥ï¼šNotion tokenã€ç½‘ç»œ/VPN/ç³»ç»Ÿä»£ç†è®¾ç½®ï¼Œæˆ–å°† session.trust_env è®¾ç½®ä¸º True ä»¥ä½¿ç”¨ç¯å¢ƒä»£ç†ã€‚")
        raise

    fetch_time = datetime.now()
    print(f"âœ… è¯»å–åˆ° {len(all_pages)} ä¸ªå¸ç§ (fetch_time={fetch_time.isoformat()})")
    return all_pages, fetch_time


def extract_symbol_data(pages: List[Dict]) -> List[Dict]:
    """æå–å¸ç§æ•°æ®ï¼šSymbol, Price Change%, Current Price"""
    symbols_data = []
    
    for page in pages:
        props = page['properties']
        
        # è·å– Symbol
        symbol_prop = props.get('Symbol', {})
        if symbol_prop.get('type') == 'title':
            texts = symbol_prop.get('title', [])
            if not texts:
                continue
            symbol = texts[0].get('plain_text', '').strip()
        else:
            continue
        
        if not symbol:
            continue
        
        # è·å– Price change
        price_change = None
        price_change_prop = props.get('Price change', {})
        if price_change_prop.get('type') == 'number':
            price_change = price_change_prop.get('number')
        
        if price_change is None:
            continue
        
        # åªæ‰«ææœ‰ Perp Price çš„å¸ç§
        perp_price = None
        perp_price_prop = props.get('Perp Price', {})
        if perp_price_prop.get('type') == 'number':
            perp_price = perp_price_prop.get('number')
        
        # å¿…é¡»æœ‰ Perp Price æ‰è®¡å…¥ç»Ÿè®¡
        if perp_price is None:
            continue
        
        symbols_data.append({
            'symbol': symbol,
            'price_change': price_change,
            'perp_price': perp_price
        })
    
    return symbols_data


def get_top_movers(symbols_data: List[Dict], top_n: int = 5) -> Dict:
    """è·å–æ¶¨è·Œå¹…å‰Nå"""
    # æŒ‰æ¶¨è·Œå¹…æ’åº
    sorted_data = sorted(symbols_data, key=lambda x: x['price_change'], reverse=True)
    
    top_gainers = sorted_data[:top_n]
    top_losers = sorted_data[-top_n:][::-1]  # åè½¬ï¼Œè®©æœ€å¤§è·Œå¹…æ’åœ¨å‰é¢
    
    return {
        'gainers': top_gainers,
        'losers': top_losers
    }


def create_daily_summary(config, top_gainers, top_losers, header_time: datetime = None):
    """åˆ›å»ºæ¯æ—¥æ€»ç»“åˆ° Notionï¼ˆä¸€æ¡è®°å½•åŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼‰"""
    
    notion_token = config['notion']['api_key']
    daily_db_id = config.get('daily_market_database_id')
    
    headers = {
        "Authorization": f"Bearer {notion_token}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }
    
    # ä½¿ç”¨ä¼ å…¥çš„è·å–æ•°æ®æ—¶é—´ä½œä¸ºè¡¨å¤´æ—¶é—´ï¼ˆfallback åˆ°å½“å‰æ—¶é—´ï¼‰
    if header_time is None:
        header_time = datetime.now()
    date_str = header_time.strftime("%Y-%m-%d %H:%M")
    
    print(f"\nğŸ“Š {date_str} è¡Œæƒ…æ€»ç»“")
    print("=" * 70)
    
    # æ„å»ºæ¶¨å¹…æ¦œæ–‡æœ¬ï¼ˆä¸åŒ…å«æ ‡é¢˜ï¼‰
    gainers_text = ""
    print("\nğŸš€ æ¶¨å¹…æ¦œ Top 5:")
    for i, item in enumerate(top_gainers, 1):
        symbol = item['symbol']
        change = item['price_change'] * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        gainers_text += f"{i}. {symbol} +{change:.2f}%\n"
        print(f"  {i}. {symbol:12s} +{change:6.2f}%")
    
    # æ„å»ºè·Œå¹…æ¦œæ–‡æœ¬ï¼ˆä¸åŒ…å«æ ‡é¢˜ï¼‰
    losers_text = ""
    print("\nğŸ“‰ è·Œå¹…æ¦œ Top 5:")
    for i, item in enumerate(top_losers, 1):
        symbol = item['symbol']
        change = item['price_change'] * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        losers_text += f"{i}. {symbol} {change:.2f}%\n"
        print(f"  {i}. {symbol:12s} {change:6.2f}%")
    
    # åˆå¹¶æˆä¸€æ¡è®°å½•
    combined_text = gainers_text + losers_text
    
    # åˆ›å»ºå•æ¡ Notion é¡µé¢
    page_data = {
        "parent": {"database_id": daily_db_id},
        "properties": {
            "Date": {
                "title": [
                    {
                        "text": {
                            "content": date_str
                        }
                    }
                ]
            },
            "æ¶¨å¹…å‰5": {
                "rich_text": [
                    {
                        "text": {
                            "content": gainers_text.strip()
                        }
                    }
                ]
            },
            "è·Œå¹…å‰5": {
                "rich_text": [
                    {
                        "text": {
                            "content": losers_text.strip()
                        }
                    }
                ]
            }
        }
    }
    
    try:
        response = requests.post(
            "https://api.notion.com/v1/pages",
            headers=headers,
            json=page_data
        )
        if response.status_code == 200:
            print(f"\nâœ… å·²å†™å…¥ Notionï¼ˆ1æ¡è®°å½•ï¼‰")
        else:
            print(f"\nâŒ å†™å…¥å¤±è´¥: {response.text}")
    except Exception as e:
        print(f"\nâŒ å†™å…¥å‡ºé”™: {str(e)}")


def main():
    print("=" * 80)
    print("ğŸ“Š æ¯æ—¥è¡Œæƒ…æ€»ç»“è„šæœ¬")
    print("=" * 80)
    
    # åŠ è½½é…ç½®
    config = load_config()
    
    notion_token = config['notion']['api_key']
    main_db_id = config['notion']['database_id']
    daily_db_id = config.get('daily_market_database_id')
    
    if not daily_db_id:
        print("âŒ æœªé…ç½®æ¯æ—¥è¡Œæƒ…æ•°æ®åº“IDï¼")
        sys.exit(1)
    
    # è¯»å–ä¸»æ•°æ®åº“ï¼Œå¹¶è·å–è¯»å–æ—¶çš„æ—¶é—´
    all_pages, fetch_time = get_all_symbols_from_notion(notion_token, main_db_id)

    # ä»…ä¿ç•™åœ¨ fetch_time å½“å¤©æœ‰æ›´æ–°çš„é¡µé¢ï¼ˆæ ¹æ® last_edited_time / created_timeï¼‰
    fetch_date_iso = fetch_time.date().isoformat()
    pages_today = []
    for p in all_pages:
        last_ts = p.get('last_edited_time') or p.get('created_time')
        if last_ts and last_ts.startswith(fetch_date_iso):
            pages_today.append(p)

    print(f"ğŸ“¥ å…±è¯»å– {len(all_pages)} ä¸ªé¡µé¢ï¼Œ{fetch_date_iso} æœ‰æ›´æ–°çš„é¡µé¢: {len(pages_today)} ä¸ª")

    # æå–æ•°æ®ï¼ˆåªä»ä»Šå¤©æœ‰æ›´æ–°çš„é¡µé¢ï¼‰
    symbols_data = extract_symbol_data(pages_today)
    print(f"ğŸ“Š æœ‰æ•ˆæ•°æ®ï¼š{len(symbols_data)} ä¸ªå¸ç§")
    
    if len(symbols_data) == 0:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼")
        sys.exit(1)
    
    # è·å–æ¶¨è·Œå¹…å‰5å
    top_movers = get_top_movers(symbols_data, top_n=5)
    
    # åˆ›å»ºæ¯æ—¥æ€»ç»“ï¼Œä½¿ç”¨ fetch_time ä½œä¸ºè¡¨å¤´æ—¶é—´
    create_daily_summary(config, top_movers['gainers'], top_movers['losers'], header_time=fetch_time)


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
